{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQLTZC64TH+qVs0zE76Ic0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siimranshahh/Python-Projects-/blob/main/Map_Reduce_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6jX8QE9O4RW",
        "outputId": "397a4114-186d-4881-f95b-f6fa114b0796"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.10/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the necessary libraries to peform the functions needed."
      ],
      "metadata": {
        "id": "GPGtSonrII_7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "zupf0hYmOd8R"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "import PyPDF2\n",
        "from spellchecker import SpellChecker"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the pages from the pdf"
      ],
      "metadata": {
        "id": "xTRC4xGjPQUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"Harry_Potter.pdf\""
      ],
      "metadata": {
        "id": "6NlY1v0MN2LC"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path, start_page, num_pages):\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for i in range(start_page, start_page + num_pages):\n",
        "            text += reader.pages[i].extract_text()\n",
        "    return text"
      ],
      "metadata": {
        "id": "jbJXmOksN5Ff"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My DOB: 4/18/2001"
      ],
      "metadata": {
        "id": "kOwlhwbUOEjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "book_start_page_date = 18\n",
        "text_date = extract_text_from_pdf(pdf_path, book_start_page_date - 1, 10)"
      ],
      "metadata": {
        "id": "s2z4qUpcN65V"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book_start_page_year = 2001\n",
        "text_year = extract_text_from_pdf(pdf_path, book_start_page_year - 1, 10)"
      ],
      "metadata": {
        "id": "UEEEszxROaTE"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return file.read()"
      ],
      "metadata": {
        "id": "a5IHzVgvNuDR"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_words(text):\n",
        "    word_list = text.lower().split()\n",
        "    return [(word.strip(\",.!?\\\"'\"), 1) for word in word_list if word.isalpha()]"
      ],
      "metadata": {
        "id": "NEKToWCbO0k8"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"file1.txt\", \"w\") as file1:\n",
        "    file1.write(text_date)\n",
        "\n",
        "with open(\"file2.txt\", \"w\") as file2:\n",
        "    file2.write(text_year)"
      ],
      "metadata": {
        "id": "VLVIR-I3Nw60"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Map Reduce to Count the words in file1:"
      ],
      "metadata": {
        "id": "xfWBqBh-IORs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return file.read()"
      ],
      "metadata": {
        "id": "bd7dKOig9JF8"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_words(text):\n",
        "    word_list = text.lower().split()\n",
        "    return [(word.strip(\",.!?\\\"'\"), 1) for word in word_list if word.isalpha()]"
      ],
      "metadata": {
        "id": "tujHzEOXO4TA"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_word_counts(mapped_data):\n",
        "    word_counts = defaultdict(int)\n",
        "    for word, count in mapped_data:\n",
        "        word_counts[word] += count\n",
        "    return word_counts\n"
      ],
      "metadata": {
        "id": "5D9LSaEeO9hC"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_file1 = read_file(\"file1.txt\")\n",
        "mapped_data = map_words(text_file1)\n",
        "word_counts = reduce_word_counts(mapped_data)"
      ],
      "metadata": {
        "id": "ZFleV_UYO-3M"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, count in word_counts.items():\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjFdBkJLPAT-",
        "outputId": "cf9561ff-7a02-45ae-d3ab-8dab69aca2d9"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rummaging: 1\n",
            "in: 14\n",
            "his: 14\n",
            "looking: 3\n",
            "for: 13\n",
            "but: 11\n",
            "he: 33\n",
            "did: 5\n",
            "seem: 1\n",
            "to: 36\n",
            "realize: 1\n",
            "was: 25\n",
            "being: 2\n",
            "because: 3\n",
            "looked: 4\n",
            "up: 6\n",
            "suddenly: 2\n",
            "at: 11\n",
            "the: 60\n",
            "which: 2\n",
            "still: 1\n",
            "staring: 1\n",
            "him: 6\n",
            "from: 3\n",
            "other: 1\n",
            "end: 1\n",
            "of: 16\n",
            "some: 2\n",
            "sight: 1\n",
            "cat: 5\n",
            "seemed: 4\n",
            "amuse: 1\n",
            "chuckled: 1\n",
            "and: 27\n",
            "should: 1\n",
            "have: 9\n",
            "found: 2\n",
            "what: 5\n",
            "inside: 2\n",
            "it: 22\n",
            "be: 14\n",
            "a: 35\n",
            "silver: 1\n",
            "cigarette: 1\n",
            "lighter: 1\n",
            "flicked: 1\n",
            "held: 1\n",
            "air: 1\n",
            "clicked: 3\n",
            "nearest: 1\n",
            "street: 4\n",
            "lamp: 2\n",
            "went: 4\n",
            "out: 6\n",
            "with: 4\n",
            "little: 5\n",
            "again: 1\n",
            "next: 3\n",
            "flickered: 1\n",
            "into: 2\n",
            "t: 20\n",
            "welve: 1\n",
            "times: 1\n",
            "until: 3\n",
            "only: 4\n",
            "lights: 1\n",
            "left: 2\n",
            "on: 11\n",
            "whole: 1\n",
            "were: 4\n",
            "two: 3\n",
            "tiny: 1\n",
            "pinpricks: 1\n",
            "eyes: 2\n",
            "watching: 2\n",
            "if: 5\n",
            "anyone: 1\n",
            "their: 2\n",
            "window: 2\n",
            "now: 3\n",
            "even: 5\n",
            "dursley: 1\n",
            "they: 3\n",
            "able: 2\n",
            "see: 2\n",
            "anything: 1\n",
            "that: 11\n",
            "happening: 1\n",
            "down: 6\n",
            "dumbledore: 12\n",
            "slipped: 1\n",
            "outer: 1\n",
            "back: 4\n",
            "cloak: 2\n",
            "set: 1\n",
            "f: 4\n",
            "toward: 1\n",
            "number: 2\n",
            "four: 2\n",
            "where: 1\n",
            "sat: 1\n",
            "wall: 3\n",
            "look: 2\n",
            "after: 1\n",
            "moment: 2\n",
            "spoke: 1\n",
            "seeing: 1\n",
            "you: 12\n",
            "professor: 20\n",
            "p: 6\n",
            "g: 6\n",
            "e: 10\n",
            "harry: 14\n",
            "potter: 10\n",
            "philosophers: 6\n",
            "stone: 6\n",
            "rowling: 6\n",
            "turned: 2\n",
            "smile: 1\n",
            "tabby: 1\n",
            "had: 11\n",
            "instead: 1\n",
            "smiling: 1\n",
            "rather: 2\n",
            "woman: 2\n",
            "who: 5\n",
            "wearing: 2\n",
            "square: 1\n",
            "glasses: 1\n",
            "exactly: 1\n",
            "shape: 1\n",
            "markings: 1\n",
            "around: 3\n",
            "its: 1\n",
            "an: 1\n",
            "emerald: 1\n",
            "her: 11\n",
            "black: 1\n",
            "hair: 1\n",
            "drawn: 1\n",
            "tight: 1\n",
            "she: 17\n",
            "distinctly: 1\n",
            "ruf: 1\n",
            "know: 8\n",
            "dear: 2\n",
            "never: 6\n",
            "seen: 2\n",
            "sit: 1\n",
            "so: 4\n",
            "stif: 2\n",
            "fly: 1\n",
            "been: 5\n",
            "sitting: 2\n",
            "brick: 1\n",
            "all: 14\n",
            "day: 5\n",
            "said: 15\n",
            "when: 3\n",
            "could: 1\n",
            "i: 14\n",
            "must: 2\n",
            "passed: 1\n",
            "dozen: 1\n",
            "feasts: 1\n",
            "parties: 1\n",
            "my: 3\n",
            "way: 2\n",
            "mcgonagall: 11\n",
            "snif: 2\n",
            "fed: 1\n",
            "angrily: 1\n",
            "s: 26\n",
            "impatiently: 1\n",
            "think: 4\n",
            "bit: 1\n",
            "more: 1\n",
            "no: 4\n",
            "muggles: 2\n",
            "noticed: 1\n",
            "going: 4\n",
            "jerked: 1\n",
            "head: 1\n",
            "dark: 1\n",
            "heard: 1\n",
            "flocks: 1\n",
            "owls: 2\n",
            "shooting: 2\n",
            "w: 2\n",
            "not: 8\n",
            "completely: 1\n",
            "bound: 1\n",
            "notice: 1\n",
            "stars: 1\n",
            "kent: 1\n",
            "bet: 1\n",
            "dedalus: 1\n",
            "much: 4\n",
            "ou: 6\n",
            "blame: 1\n",
            "gently: 1\n",
            "precious: 1\n",
            "celebrate: 1\n",
            "eleven: 2\n",
            "irritably: 1\n",
            "reason: 3\n",
            "lose: 1\n",
            "our: 2\n",
            "people: 6\n",
            "are: 6\n",
            "downright: 1\n",
            "streets: 1\n",
            "broad: 1\n",
            "dressed: 1\n",
            "muggle: 2\n",
            "swapping: 1\n",
            "threw: 1\n",
            "sideways: 1\n",
            "glance: 1\n",
            "as: 12\n",
            "though: 3\n",
            "hoping: 1\n",
            "tell: 2\n",
            "fine: 1\n",
            "thing: 1\n",
            "would: 3\n",
            "very: 3\n",
            "y: 5\n",
            "seems: 2\n",
            "disappeared: 1\n",
            "about: 4\n",
            "us: 1\n",
            "suppose: 3\n",
            "really: 1\n",
            "has: 3\n",
            "certainly: 1\n",
            "thankful: 1\n",
            "ould: 1\n",
            "care: 1\n",
            "lemon: 5\n",
            "kind: 1\n",
            "sweet: 1\n",
            "fond: 1\n",
            "thank: 1\n",
            "coldly: 1\n",
            "this: 4\n",
            "say: 1\n",
            "gone: 1\n",
            "surely: 1\n",
            "sensible: 1\n",
            "person: 1\n",
            "like: 2\n",
            "yourself: 1\n",
            "can: 4\n",
            "call: 2\n",
            "by: 3\n",
            "nonsense: 1\n",
            "years: 1\n",
            "trying: 1\n",
            "persuade: 1\n",
            "proper: 1\n",
            "v: 5\n",
            "unsticking: 1\n",
            "gets: 1\n",
            "confusing: 1\n",
            "we: 1\n",
            "keep: 1\n",
            "saying: 5\n",
            "any: 2\n",
            "frightened: 2\n",
            "sounding: 1\n",
            "half: 2\n",
            "dif: 1\n",
            "everyone: 1\n",
            "knows: 2\n",
            "one: 2\n",
            "flatter: 1\n",
            "calmly: 1\n",
            "oldemort: 2\n",
            "powers: 1\n",
            "will: 5\n",
            "too: 1\n",
            "well: 1\n",
            "noble: 1\n",
            "use: 1\n",
            "lucky: 1\n",
            "blushed: 1\n",
            "since: 1\n",
            "madam: 1\n",
            "pomfrey: 1\n",
            "told: 3\n",
            "me: 2\n",
            "liked: 1\n",
            "new: 1\n",
            "earmuf: 1\n",
            "shot: 1\n",
            "sharp: 1\n",
            "nothing: 1\n",
            "rumors: 1\n",
            "flying: 1\n",
            "why: 4\n",
            "finally: 1\n",
            "stopped: 1\n",
            "reached: 2\n",
            "point: 1\n",
            "most: 1\n",
            "anxious: 1\n",
            "real: 1\n",
            "waiting: 1\n",
            "hard: 1\n",
            "neither: 1\n",
            "nor: 1\n",
            "fixed: 1\n",
            "such: 1\n",
            "piercing: 1\n",
            "stare: 1\n",
            "plain: 1\n",
            "whatever: 1\n",
            "believe: 3\n",
            "however: 1\n",
            "choosing: 1\n",
            "another: 1\n",
            "drop: 1\n",
            "answer: 1\n",
            "pressed: 1\n",
            "last: 1\n",
            "night: 1\n",
            "hollow: 1\n",
            "find: 2\n",
            "rumor: 1\n",
            "is: 2\n",
            "lily: 1\n",
            "james: 2\n",
            "bowed: 1\n",
            "want: 1\n",
            "albus: 1\n",
            "patted: 1\n",
            "shoulder: 1\n",
            "heavily: 1\n",
            "voice: 1\n",
            "trembled: 1\n",
            "tried: 1\n",
            "kill: 4\n",
            "boy: 2\n",
            "or: 1\n",
            "how: 4\n",
            "power: 1\n",
            "somehow: 1\n",
            "broke: 1\n",
            "nodded: 1\n",
            "glumly: 1\n",
            "true: 1\n",
            "faltered: 1\n",
            "done: 1\n",
            "killed: 1\n",
            "just: 1\n",
            "astounding: 1\n",
            "things: 1\n",
            "stop: 1\n",
            "name: 1\n",
            "heaven: 1\n",
            "may: 1\n",
            "pulled: 1\n",
            "lace: 1\n",
            "handkerchief: 1\n",
            "dabbed: 1\n",
            "beneath: 1\n",
            "gave: 1\n",
            "great: 1\n",
            "took: 1\n",
            "golden: 1\n",
            "watch: 1\n",
            "pocket: 2\n",
            "examined: 1\n",
            "odd: 1\n",
            "twelve: 1\n",
            "hands: 1\n",
            "planets: 1\n",
            "moving: 1\n",
            "made: 1\n",
            "sense: 1\n",
            "put: 1\n",
            "come: 2\n",
            "bring: 1\n",
            "aunt: 2\n",
            "family: 1\n",
            "mean: 2\n",
            "live: 2\n",
            "cried: 1\n",
            "jumping: 1\n",
            "feet: 1\n",
            "pointing: 1\n",
            "them: 2\n",
            "less: 1\n",
            "got: 1\n",
            "son: 1\n",
            "saw: 1\n",
            "kicking: 1\n",
            "mother: 1\n",
            "screaming: 1\n",
            "best: 1\n",
            "place: 1\n",
            "firmly: 1\n",
            "uncle: 1\n",
            "explain: 2\n",
            "everything: 1\n",
            "older: 1\n",
            "written: 2\n",
            "letter: 1\n",
            "repeated: 1\n",
            "faintly: 1\n",
            "these: 1\n",
            "understand: 1\n",
            "famous: 3\n",
            "legend: 1\n",
            "surprised: 1\n",
            "today: 1\n",
            "known: 1\n",
            "future: 1\n",
            "there: 1\n",
            "books: 1\n",
            "every: 1\n",
            "child: 1\n",
            "world: 1\n",
            "seriously: 1\n",
            "over: 1\n",
            "top: 1\n",
            "enough: 1\n",
            "turn: 1\n",
            "before: 1\n",
            "walk: 1\n",
            "something: 2\n",
            "better: 1\n",
            "growing: 1\n",
            "away: 1\n",
            "ready: 1\n",
            "take: 1\n",
            "opened: 1\n",
            "changed: 1\n",
            "then: 1\n",
            "es: 1\n",
            "getting: 1\n",
            "eyed: 1\n",
            "thought: 1\n",
            "might: 1\n",
            "hiding: 1\n",
            "underneath: 1\n",
            "bringing: 1\n",
            "wise: 1\n",
            "trust: 2\n",
            "hagrid: 2\n",
            "important: 1\n",
            "heart: 1\n",
            "right: 1\n",
            "grudgingly: 1\n",
            "pretend: 1\n",
            "does: 1\n",
            "tend: 1\n",
            "low: 1\n",
            "rumbling: 1\n",
            "sound: 1\n",
            "broken: 1\n",
            "silence: 1\n",
            "grew: 1\n",
            "steadily: 1\n",
            "louder: 1\n",
            "sign: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counting Non English Words in File2:"
      ],
      "metadata": {
        "id": "ruZLw5R3PDrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAkXkueyPBeB",
        "outputId": "569d9a55-9b85-408c-ffbf-b650b420f055"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.10/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker"
      ],
      "metadata": {
        "id": "nJkTuj0sPoTl"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spell = SpellChecker()\n"
      ],
      "metadata": {
        "id": "ktXiJcJrPx2f"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_non_english_words(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    mapped = [(word, 1) for word in words if word.isalpha() and word not in spell]\n",
        "    return mapped"
      ],
      "metadata": {
        "id": "sG3kyG_sQe6D"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_non_english_words(mapped_data):\n",
        "    non_english_word_counts = defaultdict(int)\n",
        "    for word, count in mapped_data:\n",
        "        non_english_word_counts[word] += count\n",
        "    return non_english_word_counts\n"
      ],
      "metadata": {
        "id": "3u_GzxqHQgoA"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mapreduce_non_english_word_count(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    #Map Phase\n",
        "    mapped_data = map_non_english_words(text)\n",
        "\n",
        "    #Reduce phase\n",
        "    non_english_word_counts = reduce_non_english_words(mapped_data)\n",
        "\n",
        "    return non_english_word_counts"
      ],
      "metadata": {
        "id": "HXFWYaEVQiSH"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_english_word_counts_file2 = mapreduce_non_english_word_count('file2.txt')\n"
      ],
      "metadata": {
        "id": "kBxgSc46Qj-g"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, count in non_english_word_counts_file2.items():\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhBfGd29QnbA",
        "outputId": "af3fb371-aad9-4a65-98bd-4bde948fff94"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "veela: 11\n",
            "easley: 2\n",
            "stuf: 2\n",
            "hermione: 3\n",
            "hadn: 1\n",
            "hassan: 2\n",
            "mostafa: 7\n",
            "rowling: 6\n",
            "www: 10\n",
            "ztcprep: 10\n",
            "com: 10\n",
            "mediwizard: 1\n",
            "omnioculars: 4\n",
            "bulgarian: 6\n",
            "ther: 1\n",
            "olkov: 3\n",
            "ulchanov: 3\n",
            "ar: 2\n",
            "guing: 1\n",
            "hee: 3\n",
            "bulgarians: 2\n",
            "guments: 1\n",
            "roy: 2\n",
            "quaf: 4\n",
            "fle: 4\n",
            "bludger: 2\n",
            "dimitrov: 4\n",
            "moran: 6\n",
            "irish: 10\n",
            "atching: 1\n",
            "didn: 3\n",
            "mr: 1\n",
            "levski: 2\n",
            "ivanova: 1\n",
            "quigley: 1\n",
            "krum: 10\n",
            "couldn: 1\n",
            "ron: 3\n",
            "ime: 1\n",
            "ynch: 4\n",
            "ronski: 1\n",
            "wasn: 1\n",
            "mediwizards: 2\n",
            "ve: 2\n",
            "ou: 1\n",
            "vos: 1\n",
            "quidditch: 1\n",
            "orld: 1\n",
            "op: 2\n",
            "cornelius: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R6KfJfrkQxLA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}